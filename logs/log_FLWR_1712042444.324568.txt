FL Paper Experiment | INFO flwr 2024-04-02 10:20:47,718 | main.py:26 | 
================================================================================
FedFaces - FedAvgM(accept_failures=True) - 25 clients - IID Distribution has started
================================================================================
FL Paper Experiment | INFO flwr 2024-04-02 10:20:47,719 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=100, round_timeout=None)
FL Paper Experiment | INFO flwr 2024-04-02 10:20:50,081 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:172.18.144.40': 1.0, 'CPU': 12.0, 'node:__internal_head__': 1.0, 'memory': 8597525300.0, 'object_store_memory': 4298762649.0}
FL Paper Experiment | INFO flwr 2024-04-02 10:20:50,082 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
FL Paper Experiment | INFO flwr 2024-04-02 10:20:50,082 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 10, 'num_gpus': 0, 'memory': 3221225472}
FL Paper Experiment | INFO flwr 2024-04-02 10:20:50,089 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
FL Paper Experiment | INFO flwr 2024-04-02 10:20:50,089 | server.py:89 | Initializing global parameters
FL Paper Experiment | INFO flwr 2024-04-02 10:20:50,090 | server.py:276 | Requesting initial parameters from one random client
FL Paper Experiment | INFO flwr 2024-04-02 10:20:53,185 | server.py:280 | Received initial parameters from one random client
FL Paper Experiment | INFO flwr 2024-04-02 10:20:53,185 | server.py:91 | Evaluating initial parameters
FL Paper Experiment | INFO flwr 2024-04-02 10:20:53,185 | server.py:104 | FL starting
FL Paper Experiment | DEBUG flwr 2024-04-02 10:20:53,186 | server.py:222 | fit_round 1: strategy sampled 18 clients (out of 25)
FL Paper Experiment | DEBUG flwr 2024-04-02 10:24:10,639 | server.py:236 | fit_round 1 received 18 results and 0 failures
FL Paper Experiment | WARNING flwr 2024-04-02 10:24:17,103 | fedavgm.py:198 | No fit_metrics_aggregation_fn provided
FL Paper Experiment | DEBUG flwr 2024-04-02 10:24:17,190 | server.py:173 | evaluate_round 1: strategy sampled 18 clients (out of 25)
FL Paper Experiment | DEBUG flwr 2024-04-02 10:24:46,944 | server.py:187 | evaluate_round 1 received 18 results and 0 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:24:46,948 | server.py:222 | fit_round 2: strategy sampled 18 clients (out of 25)
FL Paper Experiment | DEBUG flwr 2024-04-02 10:28:12,749 | server.py:236 | fit_round 2 received 18 results and 0 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:28:30,022 | server.py:173 | evaluate_round 2: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:40,301 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:40,304 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:42,780 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:42,793 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:43,401 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:43,402 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:43,652 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:43,673 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:47,971 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:47,973 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:47,974 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:47,974 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:47,975 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:47,984 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,019 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,019 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,035 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,067 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,083 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,101 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,531 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:51,540 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,436 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,436 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,438 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,439 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,440 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,440 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,441 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,441 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,441 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,443 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:28:53,445 | server.py:187 | evaluate_round 2 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:28:53,447 | server.py:222 | fit_round 3: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,940 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:53,949 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:56,314 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:56,314 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:56,315 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:56,316 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:57,099 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:57,100 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:57,109 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:57,124 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:57,450 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:57,545 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:58,303 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:58,304 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:28:59,931 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:28:59,931 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,538 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,539 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,540 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,639 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,677 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:01,690 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:03,291 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:03,292 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:03,798 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:03,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:04,611 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:04,612 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:04,612 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:04,612 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:04,613 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:04,614 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:04,616 | server.py:236 | fit_round 3 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:04,664 | server.py:173 | evaluate_round 3: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:06,165 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:06,185 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:08,842 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:08,843 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:08,843 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:08,843 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:08,843 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:08,864 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,179 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,180 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,180 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,182 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,182 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,182 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,183 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,184 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,208 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:09,245 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:10,073 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:10,126 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:10,545 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:10,545 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:10,545 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:10,546 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:11,425 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:11,425 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:11,426 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:11,434 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,855 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,855 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,856 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,856 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,857 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,857 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:12,858 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:12,862 | server.py:187 | evaluate_round 3 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:12,862 | server.py:222 | fit_round 4: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:13,312 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:13,333 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:14,193 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:14,194 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:14,699 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:14,713 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:15,669 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:15,670 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:15,670 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:15,670 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:17,156 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:17,156 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:17,156 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:17,157 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:18,063 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:18,064 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:18,065 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:18,065 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:18,066 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:18,067 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:19,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:19,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,013 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,014 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,035 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,048 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,066 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,383 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,384 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,794 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,794 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,795 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,795 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,795 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:20,796 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:20,798 | server.py:236 | fit_round 4 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:20,799 | server.py:173 | evaluate_round 4: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:22,321 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:22,321 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:22,321 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:22,321 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:22,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:22,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:23,229 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:23,231 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,011 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,012 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,012 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,777 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,777 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:25,811 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,119 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,121 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,122 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,122 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,123 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,180 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,287 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:26,303 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,337 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,337 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,337 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,337 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,338 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,338 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,682 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,683 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,683 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:27,684 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:27,688 | server.py:187 | evaluate_round 4 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:27,688 | server.py:222 | fit_round 5: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:28,370 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:28,370 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:29,094 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:29,095 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,085 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,086 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,086 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,086 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,087 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,087 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,108 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:30,122 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,510 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,510 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,510 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,511 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,511 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,511 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,512 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:31,519 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:32,207 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:32,207 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:32,208 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:32,208 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,642 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,642 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,643 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,644 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,644 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,653 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,687 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,702 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,981 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,982 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,982 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:33,983 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:33,985 | server.py:236 | fit_round 5 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:33,986 | server.py:173 | evaluate_round 5: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:34,345 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:34,368 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:35,189 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:35,190 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:38,874 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:38,874 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:38,875 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:38,875 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:38,877 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:38,877 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,248 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,249 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,250 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,250 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,279 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,314 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,736 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:39,737 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,291 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,292 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,292 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,293 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,293 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,294 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,294 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,301 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,326 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,369 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,701 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:42,711 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:43,029 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:43,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:43,418 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:43,419 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:43,419 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:43,420 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:43,422 | server.py:187 | evaluate_round 5 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:43,423 | server.py:222 | fit_round 6: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:44,610 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:44,610 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:44,611 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:44,634 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:44,997 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:44,998 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:45,364 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:45,364 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:46,466 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:46,466 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:46,468 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:46,468 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:46,805 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:46,805 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:47,151 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:47,151 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:47,152 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:47,152 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:47,998 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:47,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:47,999 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:48,008 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:48,713 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:48,714 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:48,714 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:48,715 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:49,567 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:49,568 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:49,569 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:49,569 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:49,930 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:49,952 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:50,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:50,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:50,337 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:50,337 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:50,340 | server.py:236 | fit_round 6 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:50,340 | server.py:173 | evaluate_round 6: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:50,814 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:50,841 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,009 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,940 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,941 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,942 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:52,943 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,538 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,538 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,538 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,539 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,540 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,550 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,885 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:54,886 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:55,218 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:55,242 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:55,559 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:55,560 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,593 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,594 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,594 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,594 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,599 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,622 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,928 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:56,937 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:57,255 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:57,255 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:57,255 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:57,256 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:57,264 | server.py:187 | evaluate_round 6 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:29:57,264 | server.py:222 | fit_round 7: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:57,948 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:57,949 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,295 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,296 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,297 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,297 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,305 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,320 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:29:59,339 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:00,089 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:00,090 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:00,091 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:00,111 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:00,488 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:00,494 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:01,736 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:01,736 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,091 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,092 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,092 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,093 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,136 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,148 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,442 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,443 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,815 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:02,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,145 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,146 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,467 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,492 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,837 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,837 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:03,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:03,841 | server.py:236 | fit_round 7 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:03,842 | server.py:173 | evaluate_round 7: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:05,056 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:05,056 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:05,057 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:05,057 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,126 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,126 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,127 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,127 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,128 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,128 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,870 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,871 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,877 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:06,891 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:07,583 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:07,592 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,416 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,417 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,772 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,773 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,774 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,774 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,774 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:08,775 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:09,598 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:09,599 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:09,599 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:09,612 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:09,936 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:09,937 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:10,259 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:10,259 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:10,602 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:10,602 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:10,603 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:10,603 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:10,605 | server.py:187 | evaluate_round 7 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:10,606 | server.py:222 | fit_round 8: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:10,998 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:11,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:12,471 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:12,471 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:12,472 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:12,473 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:13,605 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:13,606 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:13,606 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:13,606 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:13,607 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:13,608 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,040 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,041 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,042 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,042 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,043 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,043 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,044 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,050 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,437 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:15,438 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,439 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,439 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,777 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,777 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,778 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:16,784 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:17,497 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:17,497 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:17,497 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:17,497 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:17,498 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:17,498 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:17,502 | server.py:236 | fit_round 8 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:17,502 | server.py:173 | evaluate_round 8: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:18,953 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:18,954 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:19,291 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:19,292 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:19,315 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:19,332 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:19,656 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:19,666 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:20,662 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:20,662 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:20,662 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:20,662 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:20,663 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:20,663 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,080 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,080 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,081 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,082 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,082 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,090 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,105 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,141 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,438 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:22,445 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,442 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,442 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,443 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,451 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,466 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,490 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,816 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:23,817 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:24,193 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:24,193 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:24,194 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:24,194 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:24,196 | server.py:187 | evaluate_round 8 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:24,197 | server.py:222 | fit_round 9: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:26,395 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:26,396 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,122 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,123 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,123 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,123 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,123 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,124 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,124 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,124 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,125 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,126 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,157 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,200 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,506 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:27,507 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:28,587 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:28,588 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:28,588 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:28,589 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:28,960 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:28,961 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,292 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,292 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,292 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,301 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,619 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,627 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,969 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:29,977 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:30,300 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:30,300 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:30,665 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:30,666 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:30,666 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:30,667 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:30,669 | server.py:236 | fit_round 9 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:30,670 | server.py:173 | evaluate_round 9: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,718 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,718 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,719 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,719 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,720 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,721 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,721 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,723 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,723 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:32,731 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:33,056 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:33,057 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:33,708 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:33,708 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:33,709 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:33,709 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,057 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,058 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,059 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,068 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,800 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,801 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,801 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,802 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,802 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,802 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,823 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:35,837 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,838 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,839 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,839 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,839 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,840 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,840 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,841 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:36,842 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:36,843 | server.py:187 | evaluate_round 9 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:36,844 | server.py:222 | fit_round 10: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:37,597 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:37,606 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:37,931 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:37,933 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:38,306 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:38,307 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:39,452 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:39,461 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,078 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,102 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,424 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,425 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,425 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,426 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,426 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,427 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,452 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,481 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,787 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,788 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,788 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:41,789 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:42,170 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:42,170 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:42,171 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:42,179 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,046 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,047 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,047 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,048 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,710 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,711 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,712 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,712 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,712 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:43,713 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:43,715 | server.py:236 | fit_round 10 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:43,715 | server.py:173 | evaluate_round 10: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:44,837 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:44,837 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:44,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:44,845 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:45,565 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:45,566 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:45,904 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:45,904 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:45,905 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:45,912 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:46,262 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:46,262 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:46,936 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:46,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:47,322 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:47,323 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:47,993 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:47,994 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:47,994 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:47,995 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:47,996 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:48,005 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:48,652 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:48,652 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:48,653 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:48,660 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:49,972 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:49,972 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:49,972 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:49,995 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:50,373 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:50,380 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:50,726 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:50,726 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:50,727 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:50,727 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:50,729 | server.py:187 | evaluate_round 10 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:50,729 | server.py:222 | fit_round 11: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:52,780 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:52,793 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:53,117 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:53,139 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:53,460 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:53,461 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:53,461 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:53,467 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,129 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,137 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,144 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,151 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,437 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,438 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,438 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,438 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,438 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,439 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,459 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,474 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,766 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:54,773 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:55,439 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:55,440 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,118 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,119 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,119 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,120 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,120 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,126 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,454 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,455 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,804 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,805 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,805 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:56,806 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:56,808 | server.py:236 | fit_round 11 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:30:56,808 | server.py:173 | evaluate_round 11: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:57,509 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:57,510 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:58,164 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:58,165 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:58,165 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:58,166 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:58,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:58,546 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,179 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,179 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,964 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,964 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,964 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,965 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,978 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:30:59,989 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:00,365 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:00,366 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:00,768 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:00,768 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,487 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,488 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,517 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,489 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,490 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,490 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,488 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,545 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,558 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:02,588 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,169 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,169 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,170 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,170 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,170 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,171 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:03,174 | server.py:187 | evaluate_round 11 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:03,174 | server.py:222 | fit_round 12: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,572 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:03,594 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:04,286 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:04,287 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:04,686 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:04,687 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,088 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,088 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,089 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,089 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,090 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,091 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,091 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,100 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,417 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:06,418 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,455 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,456 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,456 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,456 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,477 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,494 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,792 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:07,793 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:08,145 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:08,146 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:08,469 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:08,475 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:08,806 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:08,807 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,134 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,134 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,460 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,461 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,461 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,461 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:09,464 | server.py:236 | fit_round 12 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:09,464 | server.py:173 | evaluate_round 12: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,816 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:09,829 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:10,205 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:10,214 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:10,957 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:10,958 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:11,322 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:11,323 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:12,385 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:12,386 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:12,386 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:12,386 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:12,387 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:12,394 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:13,371 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:13,372 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:13,372 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:13,372 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:13,373 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:13,380 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:14,473 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:14,474 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:14,474 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:14,474 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:14,475 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:14,481 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,948 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,949 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,949 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,950 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,950 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,951 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,950 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,951 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,952 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:15,952 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:15,955 | server.py:187 | evaluate_round 12 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:15,956 | server.py:222 | fit_round 13: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:16,637 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:16,638 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:17,009 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:17,016 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:18,030 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:18,031 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:18,031 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:18,031 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:18,032 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:18,038 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,040 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,041 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,041 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,042 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,066 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,416 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,423 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,768 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:19,793 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:20,513 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:20,514 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:20,514 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:20,515 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:20,878 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:20,879 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:21,237 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:21,238 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,244 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,244 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,245 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,245 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,245 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,245 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,246 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:22,247 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:22,249 | server.py:236 | fit_round 13 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:22,249 | server.py:173 | evaluate_round 13: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:24,313 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:24,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:24,646 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:24,667 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,003 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,005 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,006 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,059 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,383 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:25,389 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,057 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,057 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,058 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,067 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,387 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,388 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,745 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:26,746 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:27,585 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:27,586 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:27,586 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:27,593 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:27,959 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:27,960 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:28,333 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:28,334 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:28,684 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:28,685 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:28,685 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:28,686 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:28,688 | server.py:187 | evaluate_round 13 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:28,688 | server.py:222 | fit_round 14: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:29,176 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:29,195 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:29,849 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:29,873 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,244 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,246 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,246 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,246 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,246 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,247 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,247 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,313 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,969 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,969 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,969 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:31,991 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:32,306 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:32,307 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:32,999 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:32,999 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:33,000 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:33,001 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:33,389 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:33,389 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:33,726 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:33,726 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:34,069 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:34,076 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:34,457 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:34,465 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:34,814 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:34,814 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:35,184 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:35,184 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:35,184 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:35,185 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:35,187 | server.py:236 | fit_round 14 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:35,188 | server.py:173 | evaluate_round 14: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:36,351 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:36,352 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:36,352 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:36,353 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:36,687 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:36,688 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:37,386 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:37,386 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:37,387 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:37,388 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:37,791 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:37,791 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:38,168 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:38,169 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:38,527 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:38,527 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:39,223 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:39,223 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:39,646 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:39,647 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:39,647 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:39,648 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:40,464 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:40,465 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:40,465 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:40,465 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:40,796 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:40,802 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:41,137 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:41,137 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:41,468 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:41,491 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:42,112 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:42,113 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:42,113 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:42,114 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:42,116 | server.py:187 | evaluate_round 14 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:42,117 | server.py:222 | fit_round 15: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:42,491 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:42,499 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:43,136 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:43,152 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:43,481 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:43,499 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:44,155 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:44,155 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:44,605 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:44,606 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:44,606 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:44,607 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:45,616 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:45,616 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:45,617 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:45,618 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:45,994 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:45,995 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:46,345 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:46,346 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:46,696 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:46,704 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:47,042 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:47,049 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,041 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,042 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,072 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,368 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,368 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,369 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,416 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,721 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,722 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,722 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:48,723 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:48,725 | server.py:236 | fit_round 15 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:48,725 | server.py:173 | evaluate_round 15: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,419 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,419 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,420 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,420 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,421 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,421 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,456 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:50,482 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,515 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,516 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,517 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,517 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,518 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,518 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,874 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:51,875 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:52,548 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:52,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:52,550 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:52,550 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:52,918 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:52,925 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:53,901 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:53,901 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:53,902 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:53,909 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:54,278 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:54,279 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:54,279 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:54,285 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:55,078 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:55,078 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:55,079 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:55,079 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:55,080 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:55,080 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:55,083 | server.py:187 | evaluate_round 15 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:31:55,083 | server.py:222 | fit_round 16: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,173 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,174 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,503 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,503 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,503 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,504 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,850 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:56,851 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:57,580 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:57,581 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:57,581 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:57,582 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:57,959 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:57,967 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:58,673 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:58,673 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:58,673 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:58,674 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:59,016 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:59,016 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:31:59,376 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:31:59,376 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,358 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,360 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,360 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,381 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,402 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,415 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,692 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:00,692 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:01,028 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:01,033 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:01,401 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:01,401 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:01,402 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:01,402 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:01,404 | server.py:236 | fit_round 16 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:01,405 | server.py:173 | evaluate_round 16: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:02,390 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:02,390 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:02,400 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:02,418 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:03,050 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:03,050 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:03,052 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:03,052 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:03,788 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:03,788 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:04,508 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:04,509 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:05,152 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:05,153 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:05,622 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:05,623 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:05,623 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:05,624 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,247 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,248 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,248 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,249 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,915 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,916 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,917 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,917 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,940 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:06,997 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:07,304 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:07,305 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,052 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,053 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,053 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,054 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,054 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,054 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:08,057 | server.py:187 | evaluate_round 16 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:08,057 | server.py:222 | fit_round 17: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,792 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:08,798 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:09,125 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:09,126 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,152 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,152 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,153 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,160 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,184 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,206 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,865 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,866 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,867 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:10,888 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:11,849 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:11,850 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:11,851 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:11,851 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:11,852 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:11,853 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:12,211 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:12,212 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:12,971 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:12,978 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:12,994 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:13,025 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:13,325 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:13,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,426 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,426 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,427 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,427 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,427 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,428 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,428 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,429 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:14,431 | server.py:236 | fit_round 17 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:14,432 | server.py:173 | evaluate_round 17: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,765 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:14,773 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:15,406 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:15,415 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:16,502 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:16,510 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:16,966 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:16,966 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:16,967 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:16,967 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,052 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,053 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,054 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,054 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,055 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,055 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,055 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,056 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,056 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,080 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,088 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,126 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,385 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:19,394 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:20,033 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:20,033 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:20,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:20,061 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:20,371 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:20,372 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,130 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,131 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,131 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,132 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,132 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,133 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:21,135 | server.py:187 | evaluate_round 17 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:21,135 | server.py:222 | fit_round 18: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,513 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:21,527 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:22,521 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:22,521 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:22,521 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:22,522 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:22,875 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:22,881 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,490 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,491 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,491 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,492 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,492 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,492 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,492 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,493 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,494 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,494 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,494 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,495 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,495 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,518 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,536 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,575 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,592 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,603 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,950 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,951 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,982 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:26,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:27,644 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:27,645 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:27,646 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:27,646 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:27,646 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:27,647 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:27,649 | server.py:236 | fit_round 18 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:27,650 | server.py:173 | evaluate_round 18: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:28,335 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:28,335 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,005 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,005 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,006 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,006 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,006 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:30,008 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,307 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,307 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,308 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,308 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,308 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,309 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,317 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:31,363 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,657 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,657 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,657 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,677 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:32,991 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:33,059 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:33,430 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:33,431 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:33,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:33,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:33,777 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:33,778 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:33,780 | server.py:187 | evaluate_round 18 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:33,780 | server.py:222 | fit_round 19: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:34,144 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:34,156 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:34,873 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:34,880 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:35,599 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:35,600 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:36,674 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:36,675 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:36,675 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:36,684 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,000 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,001 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,002 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,003 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,708 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,709 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,709 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:37,715 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:38,045 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:38,046 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:38,382 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:38,382 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:39,386 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:39,387 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:39,387 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:39,387 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:39,388 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:39,415 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,026 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,027 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,027 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,028 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,028 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,028 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:40,031 | server.py:236 | fit_round 19 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:40,031 | server.py:173 | evaluate_round 19: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,374 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,384 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,720 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:40,729 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:41,041 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:41,056 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:41,378 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:41,392 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:41,773 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:41,785 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:42,703 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:42,704 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:42,704 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:42,705 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:43,030 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:43,031 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,053 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,059 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,726 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,726 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,727 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,728 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,728 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,747 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,772 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:44,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:45,780 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:45,781 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:45,781 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:45,782 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:45,782 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:45,789 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:46,171 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:46,172 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:46,172 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:46,173 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:46,175 | server.py:187 | evaluate_round 19 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:46,176 | server.py:222 | fit_round 20: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:46,893 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:46,894 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:47,978 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:47,979 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:47,979 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:47,979 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:47,986 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:48,019 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:48,356 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:48,356 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:48,708 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:48,715 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:49,076 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:49,104 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:49,777 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:49,784 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,469 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,469 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,470 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,470 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,471 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,471 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,842 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:50,849 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:51,196 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:51,197 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:51,898 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:51,898 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,249 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,249 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,250 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,271 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,594 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,595 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,595 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,595 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:52,597 | server.py:236 | fit_round 20 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:52,598 | server.py:173 | evaluate_round 20: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,935 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:52,950 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:53,304 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:53,323 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:53,595 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:53,623 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:53,923 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:53,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:54,243 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:54,248 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:54,581 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:54,589 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:54,916 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:54,921 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:55,587 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:55,588 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:56,324 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:56,330 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:56,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:56,377 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:56,679 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:56,680 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:57,073 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:57,073 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,542 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,543 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,543 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,543 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,584 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,598 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,877 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,878 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,878 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:32:58,879 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:58,881 | server.py:187 | evaluate_round 20 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:32:58,882 | server.py:222 | fit_round 21: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,627 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,627 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,628 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,633 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,956 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,957 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,957 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:00,964 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,732 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,733 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,733 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,734 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,734 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,735 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,735 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,736 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,742 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,767 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,784 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:02,822 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,121 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,122 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,122 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,123 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,123 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,123 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,135 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,171 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,484 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,485 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,849 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:04,850 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:05,190 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:05,191 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:05,192 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:05,193 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-04-02 10:33:05,195 | server.py:236 | fit_round 21 received 0 results and 18 failures
FL Paper Experiment | DEBUG flwr 2024-04-02 10:33:05,195 | server.py:173 | evaluate_round 21: strategy sampled 18 clients (out of 25)
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:05,912 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:05,913 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:06,274 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:06,282 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-04-02 10:33:06,614 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-04-02 10:33:06,615 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: a2b2cdb88d74bfc3faa7ee18723386a053d79bd67e4b1fe6e92e074d) where the task (actor ID: 75262592e77a2ccbfffc6b5601000000, name=DefaultActor.__init__, pid=213014, memory used=2.70GB) was running was 14.52GB / 15.27GB (0.950823), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-6af050bdf9bd71f93c9cc90aedfd2152bcae5b6b4c1ae18f611c6a50*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
212623	8.15	python3 main.py
213014	2.70	ray::DefaultActor
212638	0.07	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
212672	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
212673	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
212737	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
212766	0.04	ray::IDLE
212763	0.04	ray::IDLE
212702	0.03	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
212701	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
