FL Paper Experiment | INFO flwr 2024-03-19 13:08:53,268 | main.py:35 | 
================================================================================
CIFAR10 - FedProx - NonIID Distribution has started
================================================================================
FL Paper Experiment | INFO flwr 2024-03-19 13:08:53,270 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=50, round_timeout=None)
FL Paper Experiment | INFO flwr 2024-03-19 13:08:55,342 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:172.18.144.40': 1.0, 'object_store_memory': 3776292864.0, 'memory': 7552585728.0, 'CPU': 6.0}
FL Paper Experiment | INFO flwr 2024-03-19 13:08:55,343 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
FL Paper Experiment | INFO flwr 2024-03-19 13:08:55,344 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0}
FL Paper Experiment | INFO flwr 2024-03-19 13:08:55,351 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 3 actors
FL Paper Experiment | INFO flwr 2024-03-19 13:08:55,352 | server.py:89 | Initializing global parameters
FL Paper Experiment | INFO flwr 2024-03-19 13:08:55,352 | server.py:276 | Requesting initial parameters from one random client
FL Paper Experiment | INFO flwr 2024-03-19 13:08:58,149 | server.py:280 | Received initial parameters from one random client
FL Paper Experiment | INFO flwr 2024-03-19 13:08:58,150 | server.py:91 | Evaluating initial parameters
FL Paper Experiment | INFO flwr 2024-03-19 13:08:58,150 | server.py:104 | FL starting
FL Paper Experiment | DEBUG flwr 2024-03-19 13:08:58,151 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 20)
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:04,845 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:04,916 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,671 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,672 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,671 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,671 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,775 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,712 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:10,802 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:11,332 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:11,333 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:11,334 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:11,335 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:11,335 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:11,336 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-19 13:11:15,889 | server.py:236 | fit_round 1 received 2 results and 8 failures
FL Paper Experiment | WARNING flwr 2024-03-19 13:11:16,330 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
FL Paper Experiment | DEBUG flwr 2024-03-19 13:11:16,331 | server.py:173 | evaluate_round 1: strategy sampled 10 clients (out of 20)
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:22,324 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:22,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:23,345 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 12 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:23,346 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 12 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:23,989 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:23,990 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:23,991 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:24,038 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,217 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 7 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,219 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 7 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,619 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,620 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,620 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,620 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,621 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,622 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,623 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,623 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,623 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:25,624 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-19 13:11:25,625 | server.py:187 | evaluate_round 1 received 0 results and 10 failures
FL Paper Experiment | DEBUG flwr 2024-03-19 13:11:25,626 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 20)
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:26,261 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:26,268 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:29,825 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:29,855 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:30,401 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:30,417 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,009 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,020 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,062 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,597 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,597 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,598 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,598 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,599 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,600 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,600 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:11:31,601 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-19 13:12:04,896 | server.py:236 | fit_round 2 received 1 results and 9 failures
FL Paper Experiment | DEBUG flwr 2024-03-19 13:12:05,016 | server.py:173 | evaluate_round 2: strategy sampled 10 clients (out of 20)
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:08,386 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:08,403 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,566 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,567 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,567 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,567 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,568 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,568 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,602 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,614 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,929 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:09,937 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,264 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,265 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,266 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,266 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,266 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,267 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,605 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 2 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:10,606 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate
    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)
  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test
    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)
AttributeError: module 'torcheval.metrics' has no attribute 'functionak'

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1340, ip=172.18.144.40, actor_id=cfda3630226e2675fc7f248401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5ecc50e710>)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 2 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/leon/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/Clients.py", line 36, in evaluate\n    loss, accuracy, f1, roc, kappa = test(self.net, self.valloader, DEVICE=self.device)\n  File "/mnt/c/Users/leon1/Documents/Research Repo/Experiments/Ethnicity FL paper/neural_nets.py", line 136, in test\n    f1 += torcheval.metrics.functionak.multiclass_f1_score(outputs, labels, classes)\nAttributeError: module \'torcheval.metrics\' has no attribute \'functionak\'\n',)
FL Paper Experiment | DEBUG flwr 2024-03-19 13:12:10,607 | server.py:187 | evaluate_round 2 received 0 results and 10 failures
FL Paper Experiment | DEBUG flwr 2024-03-19 13:12:10,607 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 20)
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:11,557 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:11,592 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:13,861 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:13,862 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-19 13:12:13,862 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: ff375edf884f6aaabe2032d401000000, name=DefaultActor.__init__, pid=1342, memory used=2.12GB) was running was 13.00GB / 13.65GB (0.951813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-871a6f70aa3d93a277cc6c3601927472df3bf2986019a3064f85d001*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1071	3.85	python3 main.py
1340	2.73	ray::DefaultActor.run
1342	2.12	ray::DefaultActor.run
1904	0.64	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
1152	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-19 13:12:13,883 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 7d47d146773a69a57f89e8a89a0f1b75c04c3dc8ecdeb92ab2bad373) where the task (actor ID: f9f076d5daf603611a67279a01000000, name=DefaultActor.__init__, pid=1341, memory used=1.39GB) was running was 12.99GB / 13.65GB (0.950962), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-aa359abd7f5d81cb9740cd8bba8887b62685a2f6c25b6c0514c19e80*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
1340	2.73	ray::DefaultActor.run
1071	2.21	python3 main.py
1342	2.00	ray::DefaultActor.run
1341	1.39	ray::DefaultActor.run
1904	0.47	ray::SPILL_SpillWorker
1183	0.05	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
1128	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
1129	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
1925	0.04	ray::IDLE_SpillWorker
1153	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
