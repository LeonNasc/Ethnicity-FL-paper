FL Paper Experiment | INFO flwr 2024-03-20 17:27:43,815 | main.py:25 | 
================================================================================
FedFaces - FedAvg(accept_failures=True) - IID Distribution has started
================================================================================
FL Paper Experiment | INFO flwr 2024-03-20 17:27:43,818 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=100, round_timeout=None)
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,981 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 6.0, 'node:172.18.144.40': 1.0, 'node:__internal_head__': 1.0, 'memory': 6561445479.0, 'object_store_memory': 3280722739.0}
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,982 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,982 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 6, 'num_gpus': 0}
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,996 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,996 | server.py:89 | Initializing global parameters
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,997 | server.py:272 | Using initial parameters provided by strategy
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,997 | server.py:91 | Evaluating initial parameters
FL Paper Experiment | INFO flwr 2024-03-20 17:27:46,997 | server.py:104 | FL starting
FL Paper Experiment | DEBUG flwr 2024-03-20 17:27:46,998 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:34,827 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:34,832 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:37,221 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:38,044 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:39,024 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:39,025 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:40,789 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:40,790 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:40,790 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:40,791 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:42,577 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:42,578 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:44,041 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:44,042 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:44,043 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:44,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-20 17:29:44,046 | server.py:236 | fit_round 1 received 2 results and 8 failures
FL Paper Experiment | WARNING flwr 2024-03-20 17:29:45,436 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
FL Paper Experiment | DEBUG flwr 2024-03-20 17:29:45,438 | server.py:173 | evaluate_round 1: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:48,962 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:48,963 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:49,582 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:49,932 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:50,086 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:50,087 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:51,276 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:51,277 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:55,892 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:55,893 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:55,893 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:55,894 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:55,894 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:55,894 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:56,683 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:56,925 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:57,080 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:57,081 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:29:57,081 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:29:57,081 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-20 17:29:57,083 | server.py:187 | evaluate_round 1 received 0 results and 10 failures
FL Paper Experiment | DEBUG flwr 2024-03-20 17:29:57,083 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,500 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,500 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,501 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,502 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,504 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,502 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,503 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,503 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,503 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:06,502 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:07,293 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:07,554 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:07,555 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:07,556 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:08,867 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:08,867 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:08,867 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:08,868 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:08,876 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:08,868 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:08,877 | server.py:236 | fit_round 2 received 0 results and 10 failures
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:08,885 | server.py:173 | evaluate_round 2: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:13,887 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:13,888 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:13,889 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:13,889 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:13,890 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:14,524 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:16,213 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:16,214 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:16,215 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:16,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:17,422 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:17,423 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,853 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,853 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,854 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,854 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:20,855 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:20,857 | server.py:187 | evaluate_round 2 received 0 results and 10 failures
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:20,857 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:22,039 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:22,663 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:23,165 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:24,206 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:25,510 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:25,511 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:30,173 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:30,173 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:30,174 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:30,174 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:30,175 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:30,827 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:31,170 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:31,173 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:32,536 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:32,536 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:32,537 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:32,536 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:32,537 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:32,538 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:32,539 | server.py:236 | fit_round 3 received 0 results and 10 failures
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:32,540 | server.py:173 | evaluate_round 3: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:35,522 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:35,905 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:36,062 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:36,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:37,195 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:37,811 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:41,737 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:41,738 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:41,738 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:41,739 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:41,739 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:41,739 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:41,740 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:42,395 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:44,040 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:44,040 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:44,040 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:44,040 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:44,041 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:44,041 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:44,044 | server.py:187 | evaluate_round 3 received 0 results and 10 failures
FL Paper Experiment | DEBUG flwr 2024-03-20 17:30:44,044 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:45,248 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:46,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 17:30:47,534 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 17:30:48,197 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: 9c9f8c96ddbc3c403c9488a513e36411196ac6669622e2fc144bc555) where the task (actor ID: b13e66428c7c917678c1e69b01000000, name=DefaultActor.__init__, pid=213121, memory used=4.46GB) was running was 13.07GB / 13.65GB (0.957206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-4b5fb0b8d7da5409dea802f2acdefb61fc007ade0934cbc98691eb1c*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
213121	4.46	ray::DefaultActor.run
212868	3.36	python3 main.py
213163	1.31	ray::SPILL_SpillWorker
213204	0.04	ray::IDLE_RestoreWorker
212960	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
213182	0.04	ray::IDLE_SpillWorker
213205	0.04	ray::IDLE_RestoreWorker
213206	0.04	ray::IDLE_RestoreWorker
212902	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
213240	0.04	ray::IDLE_RestoreWorker
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
