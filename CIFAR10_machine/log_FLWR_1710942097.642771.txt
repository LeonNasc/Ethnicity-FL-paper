FL Paper Experiment | INFO flwr 2024-03-20 15:41:39,971 | main.py:25 | 
================================================================================
CIFAR10 - FedAvg(accept_failures=True) - IID Distribution has started
================================================================================
FL Paper Experiment | INFO flwr 2024-03-20 15:41:39,974 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=100, round_timeout=None)
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,978 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 7219544064.0, 'object_store_memory': 3609772032.0, 'node:__internal_head__': 1.0, 'CPU': 6.0, 'node:172.18.144.40': 1.0}
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,978 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,978 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0}
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,987 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 3 actors
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,987 | server.py:89 | Initializing global parameters
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,988 | server.py:272 | Using initial parameters provided by strategy
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,988 | server.py:91 | Evaluating initial parameters
FL Paper Experiment | INFO flwr 2024-03-20 15:41:41,988 | server.py:104 | FL starting
FL Paper Experiment | DEBUG flwr 2024-03-20 15:41:41,989 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 10)
FL Paper Experiment | DEBUG flwr 2024-03-20 15:45:17,577 | server.py:236 | fit_round 1 received 10 results and 0 failures
FL Paper Experiment | WARNING flwr 2024-03-20 15:45:19,561 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
FL Paper Experiment | DEBUG flwr 2024-03-20 15:45:19,576 | server.py:173 | evaluate_round 1: strategy sampled 10 clients (out of 10)
FL Paper Experiment | DEBUG flwr 2024-03-20 15:45:33,363 | server.py:187 | evaluate_round 1 received 10 results and 0 failures
FL Paper Experiment | DEBUG flwr 2024-03-20 15:45:33,366 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 15:48:33,959 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 15:48:33,961 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 15:48:33,963 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 15:48:33,964 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | DEBUG flwr 2024-03-20 15:48:36,339 | server.py:236 | fit_round 2 received 8 results and 2 failures
FL Paper Experiment | DEBUG flwr 2024-03-20 15:48:37,522 | server.py:173 | evaluate_round 2: strategy sampled 10 clients (out of 10)
FL Paper Experiment | ERROR flwr 2024-03-20 15:48:42,694 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 15:48:42,695 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
FL Paper Experiment | ERROR flwr 2024-03-20 15:48:47,135 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/leon/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/leon/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

FL Paper Experiment | ERROR flwr 2024-03-20 15:48:47,155 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.18.144.40, ID: ba8d0657876c2f5fe6ed2e97b526e1cdcd2ecb8a1f3382bc6043fe83) where the task (actor ID: c93870f0992571285f308da101000000, name=DefaultActor.__init__, pid=211982, memory used=2.31GB) was running was 13.03GB / 13.65GB (0.954206), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.18.144.40`. To see the logs of the worker, use `ray logs worker-294fee756ed4d6025cf0d1066140a434f5dcefa8708391ca21a1e83e*out -ip 172.18.144.40. Top 10 memory users:
PID	MEM(GB)	COMMAND
211727	2.99	python3 main.py
211981	2.49	ray::DefaultActor.run
211982	2.31	ray::DefaultActor.run
211983	2.20	ray::DefaultActor.run
211815	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-...
211764	0.04	/usr/bin/python3 /home/leon/.local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=12...
211763	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.p...
211786	0.04	/usr/bin/python3 -u /home/leon/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --log...
211738	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/se...
211785	0.03	/home/leon/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
